{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VQMOv428DfA",
        "outputId": "f3cbd9bc-276e-4864-8a7c-763587a4b578"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5bXIvsnVQm0v"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/deep_learning_ppg/')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import architectures\n",
        "import network_training\n",
        "import plotting\n",
        "import preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "F0GDzAhFrvHV"
      },
      "outputs": [],
      "source": [
        "def read_data_from_csv(csv_file_name):\n",
        "      data = pd.read_csv(csv_file_name)\n",
        "      x_data = data.iloc[:, :-2].values\n",
        "      y_data = data.iloc[:, -2].values\n",
        "      subj = data.iloc[:, -1].values\n",
        "\n",
        "      return x_data, y_data, subj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCyTR0urJoJw"
      },
      "outputs": [],
      "source": [
        "dataset_id = 2\n",
        "\n",
        "if dataset_id == 1:\n",
        "  csv_file_name = '/content/drive/MyDrive/deep_learning_ppg/data_wesad.csv'\n",
        "  labels = ('baseline', 'stress', 'amusement', 'meditation')\n",
        "  avg = False\n",
        "elif dataset_id == 2:\n",
        "  csv_file_name = '/content/drive/MyDrive/deep_learning_ppg/data_affectiveroad.csv'\n",
        "  labels = ('low', 'medium', 'high')\n",
        "  avg = True\n",
        "\n",
        "features, targets, subj_data = read_data_from_csv(csv_file_name)\n",
        "fs = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0delxoDwp0D"
      },
      "outputs": [],
      "source": [
        "# Use data from one wrist only: 0 - left, 1 - right\n",
        "\n",
        "wrist_id = 0\n",
        "features = features[:, wrist_id].reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTv6Q3Ubawlw"
      },
      "outputs": [],
      "source": [
        "params = {\n",
        "    \"seed\" : 256,\n",
        "    \"learning_rate\" : 0.0001,\n",
        "    \"weight_decay\" : 1e-6,\n",
        "    \"step_size\" : 3,\n",
        "    \"gamma\" : 0.85,\n",
        "    \"batch_size\" : 128,\n",
        "    \"epochs\" : 30,\n",
        "    \"num_resblocks\" : 5,\n",
        "    \"num_lstm_layers\" : 2,\n",
        "    \"num_lstm_units\" : 128,\n",
        "    \"resblock_id\" : 4\n",
        "}\n",
        "\n",
        "params[\"window_size\"] = fs * 8\n",
        "params[\"overlap\"] = params[\"window_size\"] * 7//8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzwYFHnv_J4I"
      },
      "outputs": [],
      "source": [
        "scaled_data = preprocessing.scale_data(features, targets)\n",
        "\n",
        "sliding_X_data, sliding_y_data = preprocessing.apply_sliding_window(scaled_data, targets, subj_data, params[\"window_size\"], params[\"overlap\"], avg)\n",
        "\n",
        "X_data = sliding_X_data.astype(np.float32)\n",
        "y_data = sliding_y_data.astype(np.uint8)\n",
        "\n",
        "print(X_data.shape, y_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItuhNLcn_3VR"
      },
      "outputs": [],
      "source": [
        "params[\"num_channels\"] = X_data.shape[2]\n",
        "params[\"num_classes\"] = len(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovyf40HS__og"
      },
      "outputs": [],
      "source": [
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def training_loop(X_data, y_data, params, architecture_id, crossvalid = False):\n",
        "  if crossvalid:\n",
        "    num_folds = 5\n",
        "    kf = KFold(n_splits=num_folds, shuffle=True, random_state = params[\"seed\"])\n",
        "    data_splits = [(X_data[train_ind], X_data[val_ind], y_data[train_ind], y_data[val_ind]) for train_ind, val_ind in kf.split(X_data)]\n",
        "  else:\n",
        "    num_folds = 1\n",
        "    X_train, X_val, y_train, y_val = network_training.split_data(X_data, y_data, train_size=0.8)\n",
        "    data_splits = [(X_train, X_val, y_train, y_val)]\n",
        "\n",
        "  avg_losses_train = []\n",
        "  f1_scores_train = []\n",
        "  accuracies_train = []\n",
        "  recalls_train = []\n",
        "  precisions_train = []\n",
        "\n",
        "  avg_losses_val = []\n",
        "  f1_scores_val = []\n",
        "  accuracies_val = []\n",
        "  recalls_val = []\n",
        "  precisions_val = []\n",
        "\n",
        "  for fold, (X_train_fold, X_val_fold, y_train_fold, y_val_fold) in enumerate(data_splits):\n",
        "    print(f\"Fold {fold+1}/{num_folds}\")\n",
        "\n",
        "    if architecture_id == 1:\n",
        "                network = architectures.DeepConvLSTM(params[\"num_channels\"], params[\"num_classes\"], num_layers=params[\"num_lstm_layers\"], hidden_size=params[\"num_lstm_units\"])\n",
        "    else:\n",
        "                network = architectures.ResNet(params[\"num_channels\"], params[\"num_classes\"], num_resblocks=params[\"num_resblocks\"], opt=params[\"resblock_id\"])\n",
        "\n",
        "\n",
        "    if fold == 0: print(network)\n",
        "\n",
        "    optimizer = torch.optim.Adam(network.parameters(), lr=params[\"learning_rate\"], weight_decay=params[\"weight_decay\"])\n",
        "    class_weights = preprocessing.get_class_weights(y_data)\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    scheduler = lr_scheduler.StepLR(optimizer, step_size=params[\"step_size\"], gamma=params[\"gamma\"])\n",
        "\n",
        "    trainloader, valloader = network_training.data_loader(X_train_fold, y_train_fold,\n",
        "                                                                  X_val_fold, y_val_fold, params[\"batch_size\"])\n",
        "\n",
        "    for epoch in range(params[\"epochs\"]):\n",
        "      print(f\"Epoch {epoch + 1}:\")\n",
        "      avg_loss_train, f1_train, accuracy_train, precision_train, recall_train = \\\n",
        "           network_training.train(trainloader, network, optimizer, criterion, scheduler)\n",
        "      avg_loss_val, f1_val, accuracy_val, precision_val, recall_val = \\\n",
        "           network_training.valid(valloader, network, criterion)\n",
        "\n",
        "    avg_losses_train.append(avg_loss_train)\n",
        "    f1_scores_train.append(f1_train)\n",
        "    accuracies_train.append(accuracy_train)\n",
        "    recalls_train.append(recall_train)\n",
        "    precisions_train.append(precision_train)\n",
        "\n",
        "    avg_losses_val.append(avg_loss_val)\n",
        "    f1_scores_val.append(f1_val)\n",
        "    accuracies_val.append(accuracy_val)\n",
        "    recalls_val.append(recall_val)\n",
        "    precisions_val.append(precision_val)\n",
        "\n",
        "  mean_avg_losses_train = np.mean(avg_losses_train)\n",
        "  mean_f1_scores_train = np.mean(f1_scores_train)\n",
        "  mean_accuracies_train = np.mean(accuracies_train)\n",
        "  mean_recalls_train = np.mean(recalls_train)\n",
        "  mean_precisions_train = np.mean(precisions_train)\n",
        "\n",
        "  mean_avg_losses_val = np.mean(avg_losses_val)\n",
        "  mean_f1_scores_val = np.mean(f1_scores_val)\n",
        "  mean_accuracies_val = np.mean(accuracies_val)\n",
        "  mean_recalls_val = np.mean(recalls_val)\n",
        "  mean_precisions_val = np.mean(precisions_val)\n",
        "\n",
        "  print(f\"\"\"Train: F1 - {mean_f1_scores_train*100:.2f} acc - {mean_accuracies_train*100:.2f}\n",
        "        prec - {mean_precisions_train*100:.2f} rec - {mean_recalls_train*100:.2f}\n",
        "        loss - {mean_avg_losses_train:.2f}\"\"\")\n",
        "  print(f\"\"\"Valid: F1 - {mean_f1_scores_val*100:.2f} acc -  {mean_accuracies_val*100:.2f}\n",
        "        prec - {mean_precisions_val*100:.2f}  rec - {mean_recalls_val*100:.2f}\n",
        "        loss - {mean_avg_losses_val:.2f}\"\"\")\n",
        "  return network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qPPibFaEnDb"
      },
      "outputs": [],
      "source": [
        "np.random.seed(params[\"seed\"])\n",
        "torch.manual_seed(params[\"seed\"])\n",
        "\n",
        "# LSTM - 1, ResNet - 2\n",
        "architecture_id = 2\n",
        "model = training_loop(X_data, y_data, params, architecture_id, crossvalid=False)\n",
        "\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Number of parameters in the model: {num_params}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}